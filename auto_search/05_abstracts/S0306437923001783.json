{"pii": "S0306437923001783", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "d1e3439", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e3440"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "d1e3442", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e3443", "view": "all"}, "_": "With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models \u2013 including modern Transformers \u2013 get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique \u2013 in place of the worst \u2013 can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Na\u00efve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing."}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "id": "d1e3445", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e3446"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "d1e3448", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e3449", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "d1e3451"}, "$$": [{"#name": "list-item", "$": {"id": "d1e3452"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3455", "view": "all"}, "_": "The text preprocessing techniques available in the literature are discussed."}]}, {"#name": "list-item", "$": {"id": "d1e3457"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3460", "view": "all"}, "_": "The impact of the three most common techniques on SOTA models is evaluated."}]}, {"#name": "list-item", "$": {"id": "d1e3462"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3465", "view": "all"}, "_": "Text preprocessing can significantly affect the performance of Transformers."}]}, {"#name": "list-item", "$": {"id": "d1e3467"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3470", "view": "all"}, "_": "Traditional classifiers can outperform Transformers, using appropriate preprocessing."}]}, {"#name": "list-item", "$": {"id": "d1e3472"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3475", "view": "all"}, "_": "The proper preprocessing should be based on the models and datasets considered."}]}]}]}]}]}]}}