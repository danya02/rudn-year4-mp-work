{"pii": "S0093934X14001515", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author-highlights", "lang": "en", "id": "ab005", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st160"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp005", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "l0005"}, "$$": [{"#name": "list-item", "$": {"id": "u0005"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0355", "view": "all"}, "_": "The amount of information conveyed by each word in a sentence can be quantified by probabilistic models of language."}]}, {"#name": "list-item", "$": {"id": "u0010"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0360", "view": "all"}, "_": "We compare such information measures to ERP amplitudes across a number of naturally occurring sentences."}]}, {"#name": "list-item", "$": {"id": "u0015"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0365", "view": "all"}, "_": "A word\u2019s unexpectedness, quantified as its surprisal, significantly predicts N400 size."}]}, {"#name": "list-item", "$": {"id": "u0020"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0370", "view": "all"}, "_": "Model comparison reveals that readers\u2019 expectations about upcoming words do not rely on hierarchical sentence structure."}]}]}]}]}]}, {"#name": "abstract", "$": {"class": "author", "lang": "en", "id": "ab010", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st165"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as010", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp010", "view": "all"}, "_": "Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence\u2019s hierarchical structure for generating expectations about the upcoming word."}]}]}]}}