{"pii": "S2352711020303277", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e168"}, "$$": [{"#name": "section-title", "$": {"id": "d1e169"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e171"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e172"}, "$$": [{"#name": "monospace", "_": "RankEval"}, {"#name": "__text__", "_": " is a Python open-source tool for the "}, {"#name": "italic", "_": "analysis and evaluation"}, {"#name": "__text__", "_": " of ranking models based on ensembles of decision trees. Learning-to-Rank ("}, {"#name": "monospace", "_": "LtR"}, {"#name": "__text__", "_": ") approaches that generate tree-ensembles are considered the most effective solution for difficult ranking tasks and several impactful "}, {"#name": "monospace", "_": "LtR"}, {"#name": "__text__", "_": " libraries have been developed aimed at improving ranking quality and training efficiency. However, these libraries are not very helpful in terms of hyper-parameters tuning and in-depth analysis of the learned models, and even the implementation of most popular Information Retrieval (IR) metrics differ among them, thus making difficult to compare different models. "}, {"#name": "monospace", "_": "RankEval"}, {"#name": "__text__", "_": " overcomes these limitations by providing a unified environment where to perform an easy, comprehensive inspection and assessment of ranking models trained using different machine learning libraries. The tool focuses on ensuring efficiency, flexibility and extensibility and is fully interoperable with most popular "}, {"#name": "monospace", "_": "LtR"}, {"#name": "__text__", "_": " libraries."}]}]}]}]}}