{"pii": "S0950705122002416", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e2634"}, "$$": [{"#name": "section-title", "$": {"id": "d1e2635"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e2637"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e2638"}, "_": "Modern cross-lingual document retrieval models are capable of finding documents relevant to the query. However, they do not have the capabilities for explaining why the document is relevant. This paper proposes a novel learning-to-rank model named LM-EMD that uses the multilingual BERT language model and Earth Mover\u2019s Distance (EMD) to measure the document\u2019s relevancy to the input query and provide interpretable insights into why a document is relevant. The model uses the query and document token\u2019s contextual embeddings generated with multilingual BERT to measure their distances in the embedding space, which are then used by EMD to calculate the document\u2019s relevance score and identify which document tokens contribute the most to its relevancy. We evaluate the model on five language pairs of varying degrees of similarity and analyze its performance. We find that the model (1) performs similar as the best performing comparing model on high-resource languages, (2) is less effective on low-resource languages, and (3) provides insight into why a document is relevant to the query."}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "view": "all", "id": "d1e2640"}, "$$": [{"#name": "section-title", "$": {"id": "d1e2641"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e2643"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e2644"}, "$$": [{"#name": "list", "$": {"id": "d1e2646"}, "$$": [{"#name": "list-item", "$": {"id": "d1e2647"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2650"}, "_": "Theoretical background on using optimal transport for measuring document relevancy."}]}, {"#name": "list-item", "$": {"id": "d1e2652"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2655"}, "_": "A novel learning-to-rank model outputting interpretable document relevance scores."}]}, {"#name": "list-item", "$": {"id": "d1e2657"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2660"}, "_": "The model performs best on high-resource language pair data sets."}]}, {"#name": "list-item", "$": {"id": "d1e2662"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2665"}, "_": "Performs an analysis of the impact the loss function has on the model\u2019s performance."}]}]}]}]}]}]}}