{"pii": "S2772485922000503", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e513"}, "$$": [{"#name": "section-title", "$": {"id": "d1e514"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e516"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e517"}, "$$": [{"#name": "__text__", "_": "Scientific research communities are embracing AI-based solutions to target tractable scientific tasks and improve research work flows. However, the development and evaluation of such solutions are scattered across multiple disciplines. We formalize the problem of scientific AI benchmarking, and propose a system called SAIBench in the hope of unifying the efforts and enabling low-friction on-boarding of new disciplines. The system approaches this goal with "}, {"#name": "italic", "_": "SAIL"}, {"#name": "__text__", "_": ", a domain-specific language to decouple research problems, AI models, ranking criteria, and software/hardware configuration into reusable modules. We show that this approach is flexible and can adapt to problems, AI models, and evaluation methods defined in different perspectives. The project homepage is "}, {"#name": "inter-ref", "$": {"xmlns:xlink": true, "id": "interref1", "href": "https://www.computercouncil.org/SAIBench", "type": "simple"}, "_": "https://www.computercouncil.org/SAIBench"}, {"#name": "__text__", "_": "."}]}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "view": "all", "id": "d1e534"}, "$$": [{"#name": "section-title", "$": {"id": "d1e535"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e537"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e538"}, "$$": [{"#name": "list", "$": {"id": "d1e540"}, "$$": [{"#name": "list-item", "$": {"id": "d1e541"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e544"}, "_": "We present a way to categorize tractable scientific tasks, and formulate the definition of scientific AI benchmarking."}]}, {"#name": "list-item", "$": {"id": "d1e546"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e549"}, "_": "We propose a DSL so that various components (AI models, problem definitions etc.) can be modularly implemented."}]}, {"#name": "list-item", "$": {"id": "d1e551"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e554"}, "_": "We propose a system design, so that various components can be automatically combined for benchmarking."}]}]}]}]}]}]}}