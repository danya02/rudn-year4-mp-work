{"pii": "S0168169924006598", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author-highlights", "lang": "en", "id": "ab005", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st005"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0005", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "l0005"}, "$$": [{"#name": "list-item", "$": {"id": "o0005"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0005", "view": "all"}, "_": "Agriculture-BERT is compared to other BERT models on agricultural term extraction."}]}, {"#name": "list-item", "$": {"id": "o0010"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0010", "view": "all"}, "_": "With a few exceptions, Agriculture-BERT performs better than the other BERT models."}]}, {"#name": "list-item", "$": {"id": "o0015"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0015", "view": "all"}, "_": "Embedding\u00a0+\u00a0encoder layers update scores best for extracting terms seen in training."}]}, {"#name": "list-item", "$": {"id": "o0020"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0020", "view": "all"}, "_": "Embedding layer frozen\u00a0+\u00a0encoder layers updated scores best for extracting synonyms."}]}, {"#name": "list-item", "$": {"id": "o0025"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0025", "view": "all"}, "_": "Embedding layer\u00a0+\u00a0top 4 encoder layers updated scores best for novel term extraction."}]}]}]}]}]}, {"#name": "abstract", "$": {"class": "author", "id": "ab010", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st010"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as010", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0010", "view": "all"}, "_": "This paper compares different transformer-based language models for automatic term extraction from agriculture-related texts. Agriculture is an important economic sector faced with severe environmental and societal challenges. The collection, annotation and sharing of agricultural scientific knowledge is key to enabling the agricultural sector to address its challenges. Automatic term extraction is a Natural Language Processing task that can provide solutions to text tagging and annotation towards better knowledge and information exchange. It is concerned with the identification of terms pertaining to a domain, or area of expertise, in text and is an important step in knowledge base creation and update pipelines. Transformer-based language modeling technologies like BERT have become popular for automatic term extraction, but limited work has been undertaken so far in applying these methods to agriculture. This paper systematically compares Agriculture-BERT to Sci-BERT, RoBERTa, and vanilla BERT, which were fine-tuned for the automatic extraction of agricultural terms from English texts. The greatest challenge faced in our research was the scarcity of agriculture-related gold standard corpora for measuring automatic term extraction performance. Our results show that, with a few exceptions, Agriculture-BERT performs better than the other models considered in our research. Our main contribution and novelty of the presented research is the investigation of the impact that different language model fine-tuning configuration scenarios had on the term extraction task. More specifically, we tested different scenarios related to the model layers kept frozen, or being updated, during training, to measure the impact they may have on Agriculture-BERT\u2019s performance in automatic term extraction. Our results show that the best performance was achieved by: (i) the \u201cembedding layer updated\u00a0+\u00a0all encoder layers updated\u201d scenario for the identification of terms also seen during training; (ii) the \u201cembedding layer frozen\u00a0+\u00a0all encoder layers updated\u201d scenario for the identification of terms being synonyms to those seen during training; and (iii) the \u201cembedding layer updated\u00a0+\u00a0top 4 encoder layers updated\u201d scenario for identifying terms neither seen during training nor being synonyms to those seen during training (novel terms)."}]}]}]}}