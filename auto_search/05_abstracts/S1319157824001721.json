{"pii": "S1319157824001721", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "ab005", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st005"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0005", "view": "all"}, "_": "One of the crucial pre-processing stages in data mining and machine learning is feature selection, which is used to choose a subset of representative characteristics and decrease dimensions. By eliminating unnecessary and redundant features, feature selection can improve machine learning tasks\u2019 accuracy. This work presents a novel multi-label classification (MLC) model utilizing a combination of stack regression (RR) and original label space transformation (IPLST) called RR-IPLST (original label space transformation-ridge regression). A novel embedded technique is implemented, utilizing competitive crowding optimizer (CSO) for multi-label feature selection. Particles are first created using this procedure, after which they are split into two equal groups and compete in pairs. The winners advance to the next iteration, while the losers pick up tips from the victors. At the conclusion of each iteration, the objective function for every particle is determined. A local search technique inspired by the gradient descent algorithm is used to find the local structure of the data, and half of the initial population is produced by the similarity between features and labels in order to boost the convergence rate. Ultimately, feature selection is carried out depending on the best particle. Six popular and sophisticated multi-label feature selection techniques are evaluated to see how well the suggested approach performs. According to the simulation results, the application of the suggested solution performs better than comparable techniques in terms of stability, accuracy, precision, convergence, error measurement, and other criteria that have been examined on various data sets. In 93.35% of cases, the test results demonstrate superiority over traditional algorithms."}]}]}]}}