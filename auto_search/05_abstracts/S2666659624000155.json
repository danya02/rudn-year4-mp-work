{"pii": "S2666659624000155", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"id": "abs0001", "view": "all", "class": "author"}, "$$": [{"#name": "section-title", "$": {"id": "cesectitle0001"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "abss0001", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "spara007", "view": "all"}, "_": "Automated Facial Analysis technologies, predominantly used for facial detection and recognition, have garnered significant attention in recent years. Although these technologies have seen advancements and widespread adoption, biases embedded within systems have raised ethical concerns. This research aims to delve into the disparities of Automatic Gender Recognition systems (AGRs), particularly their oversimplification of gender identities through a binary lens. Such a reductionist perspective is known to marginalize and misgender individuals. This study set out to investigate the alignment of an individual's gender identity and its expression through the face with societal norms, and the perceived difference between misgendering experiences from machines versus humans. Insights were gathered through an online survey, utilizing an AGR system to simulate misgendering experiences. The overarching goal is to shed light on gender identity nuances and guide the creation of more ethically responsible and inclusive facial recognition software."}]}]}]}}