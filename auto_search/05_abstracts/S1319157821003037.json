{"pii": "S1319157821003037", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "lang": "en", "id": "ab005", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st165"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp005", "view": "all"}, "_": "The Web is fed with experiences about phishing, which aim at sharing phisher techniques and behaviours. We argue that this textual information can be turned into knowledge, exploitable to prevent such attacks. Unlike anti-phishing works that aim at detecting phishing traces, this work is the first attempt to design a tool to retrieve web pages which have phishing contents. The expected crawler is dedicated to extract phishing feeds. Existing crawlers mainly rely on building vector space models (VSM) from pages while exploiting Term frequency inverse - document frequency (Tf-idf) and cosine similarity to compute Web page similarities related to a given query. Considering the fact that vector modelling ignores the order of appearance of terms in the document as well as proximity and the connections between terms, we introduce Crawl-shing, an improved search model based on isomorphic graphs, which given two documents evaluate their similarity degree by seeking the largest common subgraph. Experimental results with phishing Web pages show that Crawl-shing presents a harvest rate better than the Breadth First Search (BFS) approach. Crawl-shing has been found more precise during exploration compared to the approaches based on vector modelling."}]}]}]}}