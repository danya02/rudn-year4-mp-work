{"pii": "S2666389922000733", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"id": "abs0010", "view": "all", "class": "author"}, "$$": [{"#name": "section-title", "$": {"id": "sectitle0010"}, "_": "Summary"}, {"#name": "abstract-sec", "$": {"id": "abssec0010", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "abspara0010", "view": "all"}, "$$": [{"#name": "__text__", "_": "A bottleneck in efficiently connecting new materials discoveries to established literature has arisen due to an increase in publications. This problem may be addressed by using named entity recognition (NER) to extract structured summary-level data from unstructured materials science text. We compare the performance of four NER models on three materials science datasets. The four models include a bidirectional long short-term memory (BiLSTM) and three transformer models (BERT, SciBERT, and MatBERT) with increasing degrees of domain-specific materials science pre-training. MatBERT improves over the other two BERT"}, {"#name": "inf", "$": {"loc": "post"}, "_": "BASE"}, {"#name": "__text__", "_": "-based models by 1%\u223c12%, implying that domain-specific pre-training provides measurable advantages. Despite relative architectural simplicity, the BiLSTM model consistently outperforms BERT, perhaps due to its domain-specific pre-trained word embeddings. Furthermore, MatBERT and SciBERT models outperform the original BERT model to a greater extent in the small data limit. MatBERT\u2019s higher-quality predictions should accelerate the extraction of structured data from materials science literature."}]}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "id": "abs0015", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "sectitle0015"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "abssec0015", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "abspara0015", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "ulist0010"}, "$$": [{"#name": "list-item", "$": {"id": "u0010"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0010", "view": "all"}, "_": "Efficient extraction of information from materials science literature is needed"}]}, {"#name": "list-item", "$": {"id": "u0015"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0015", "view": "all"}, "_": "Domain-specific materials science pre-training improves results"}]}, {"#name": "list-item", "$": {"id": "u0020"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0020", "view": "all"}, "_": "Even simpler domain-specific models can outperform more complex general models"}]}]}]}]}]}, {"#name": "abstract", "$": {"class": "editor-highlights", "id": "abs0020", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "sectitle0020"}, "_": "The bigger picture"}, {"#name": "abstract-sec", "$": {"id": "abssec0020", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "abspara0020", "view": "all"}, "_": "A bottleneck in efficiently connecting new materials discoveries to established literature has arisen due to a massive increase in publications. Four different language models are trained to automatically collect important information from materials science articles. We compare a simple model (BiLSTM) with materials science knowledge to three variants of a more complex model: one with general knowledge (BERT), one with general scientific knowledge (SciBERT), and one with materials science knowledge (MatBERT). We find that MatBERT performs the best overall. This implies that language models with greater extents of materials science knowledge will perform better on materials science-related tasks. The simpler model even consistently outperforms BERT. Furthermore, the performance gaps grow when the models are given fewer examples of information extraction to learn from. MatBERT\u2019s higher-quality results should accelerate the collection of information from materials science literature."}]}]}, {"#name": "abstract", "$": {"class": "teaser", "id": "abs0025", "view": "all"}, "$$": [{"#name": "abstract-sec", "$": {"id": "abssec0025", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "abspara0025", "view": "all"}, "_": "Efficient automated extraction of information from materials science literature is needed due to an increasingly unwieldy number of publications. For a selection of materials science NER tasks, Trewartha et\u00a0al. find that language models pre-trained on materials science literature provide measurable advantages over language models pre-trained on general literature or even scientific literature from multiple fields. This provides an opportunity to produce higher-quality results that require less training data in order to address this problem."}]}]}]}}