{"pii": "S1566253523004104", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "d1e549", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e550"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "d1e552", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e553", "view": "all"}, "$$": [{"#name": "__text__", "_": "Robustness has become an important consideration in deep learning. With the help of explainable AI, mismatches between an explained model\u2019s decision strategy and the user\u2019s domain knowledge (e.g. Clever Hans effects) have been identified as a starting point for improving faulty models. However, it is less clear what to do when the "}, {"#name": "italic", "_": "user and the explanation agree"}, {"#name": "__text__", "_": ". In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a machine learning model to be robust against Clever Hans effects, which may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that "}, {"#name": "italic", "_": "preemptively"}, {"#name": "__text__", "_": " prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve higher accuracy on new data."}]}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "id": "d1e561", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e562"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "d1e564", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e565", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "d1e567"}, "$$": [{"#name": "list-item", "$": {"id": "d1e568"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e571", "view": "all"}, "_": "Explainable AI may not detect all Clever Hans effects in practical ML scenarios."}]}, {"#name": "list-item", "$": {"id": "d1e573"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e576", "view": "all"}, "_": "Hidden Clever Hans effects can be mitigated using Exposure Minimization."}]}, {"#name": "list-item", "$": {"id": "d1e578"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e581", "view": "all"}, "_": "We propose a solution in the form of a soft-pruning scheme on network activations."}]}, {"#name": "list-item", "$": {"id": "d1e583"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e586", "view": "all"}, "_": "We demonstrate the practical benefits of our approach on image and text data."}]}]}]}]}]}]}}