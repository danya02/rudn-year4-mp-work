{"pii": "S2949719123000250", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e425"}, "$$": [{"#name": "section-title", "$": {"id": "d1e426"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e428"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e429"}, "_": "Automated text summarization systems require to be heedful of the reader and the communication goals since it may be the determining component of whether the original textual content is actually worth reading in full. The summary can also assist enhance document indexing for information retrieval, and it is generally much less biased than a human-written summary. A crucial part while building intelligent systems is evaluating them. Consequently, the choice of evaluation metric(s) is of utmost importance. Current standard evaluation metrics like BLEU and ROUGE, although fairly effective for evaluation of extractive text summarization systems, become futile when it comes to comparing semantic information between two texts, i.e in abstractive summarization. We propose textual entailment as a potential metric to evaluate abstractive summaries. The results show the contribution of text entailment as a strong automated evaluation model for such summaries. The textual entailment scores between the text and generated summaries, and between the reference and predicted summaries were calculated, and an overall summarizer score was generated to give a fair idea of how efficient the generated summaries are. We put forward some novel methods that use the entailment scores and the final summarizer scores for a reasonable evaluation of the same across various scenarios. A Final Entailment Metric Score (FEMS) was generated to get an insightful idea in order to compare both the generated summaries."}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "view": "all", "id": "d1e431"}, "$$": [{"#name": "section-title", "$": {"id": "d1e432"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e434"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e435"}, "$$": [{"#name": "list", "$": {"id": "d1e437"}, "$$": [{"#name": "list-item", "$": {"id": "d1e438"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e441"}, "_": "We propose textual entailment as a metric to evaluate abstractive summaries."}]}, {"#name": "list-item", "$": {"id": "d1e443"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e446"}, "_": "The summary-based entailment model performed well in the BART model."}]}, {"#name": "list-item", "$": {"id": "d1e448"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e451"}, "_": "The mutual entailment scores had some insightful findings as well."}]}, {"#name": "list-item", "$": {"id": "d1e453"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e456"}, "_": "The results show that text entailment is a strong automated evaluation model"}]}, {"#name": "list-item", "$": {"id": "d1e458"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e461"}, "_": "A mix of Entailment and final summarizer scores is used to evaluate across scenarios."}]}, {"#name": "list-item", "$": {"id": "d1e463"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e466"}, "_": "Final Entailment Metric Score was generated to compare both the generated summaries."}]}]}]}]}]}]}}