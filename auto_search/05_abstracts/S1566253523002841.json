{"pii": "S1566253523002841", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "d1e3360", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e3361"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "d1e3363", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e3364", "view": "all"}, "$$": [{"#name": "__text__", "_": "Recent days witness significant progress in various multi-modal tasks made by Contrastive Language-Image Pre-training (CLIP), a multi-modal large-scale model that learns visual representations from natural language supervision. However, the potential effects of CLIP on cross-modal hashing retrieval has not been investigated yet. In this paper, we for the first time explore the effects of CLIP on cross-modal hashing retrieval performance and propose a simple but strong baseline Unsupervised Contrastive Multi-modal Fusion Hashing network (UCMFH). We first extract the off-the-shelf visual and linguistic features from the CLIP model, as the input sources for cross-modal hashing functions. To further mitigate the semantic gap between the image and text features, we design an effective contrastive multi-modal learning module that leverages a multi-modal fusion transformer encoder supervising by a contrastive loss, to enhance modality interaction while improving the semantic representation of each modality. Furthermore, we design a contrastive hash learning module to produce high-quality modal-correlated hash codes. Experiments show that significant performance improvement can be made by our simple new unsupervised baseline UCMFH compared with state-of-the-art supervised and unsupervised cross-modal hashing methods. Also, our experiments demonstrate the remarkable performance of CLIP features on cross-modal hashing retrieval task compared to deep visual and linguistic features used in existing state-of-the-art methods. The source codes for our approach is publicly available at: "}, {"#name": "inter-ref", "$": {"xmlns:xlink": true, "id": "interref1", "href": "https://github.com/XinyuXia97/UCMFH", "type": "simple"}, "_": "https://github.com/XinyuXia97/UCMFH"}, {"#name": "__text__", "_": "."}]}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "id": "d1e3369", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e3370"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "d1e3372", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e3373", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "d1e3375"}, "$$": [{"#name": "list-item", "$": {"id": "d1e3376"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3379", "view": "all"}, "_": "Explore the effects of scaling multimodal CLIP model for cross-modal retrieval."}]}, {"#name": "list-item", "$": {"id": "d1e3381"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3384", "view": "all"}, "_": "Propose a novel unsupervised contrastive multi-modal fusion hashing network."}]}, {"#name": "list-item", "$": {"id": "d1e3386"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3389", "view": "all"}, "_": "The novel network performs better than SOTA unsupervised and supervised methods."}]}, {"#name": "list-item", "$": {"id": "d1e3391"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "d1e3394", "view": "all"}, "_": "CLIP features can achieve significant improvement compared with CNN/BoW features."}]}]}]}]}]}]}}