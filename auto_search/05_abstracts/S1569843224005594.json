{"pii": "S1569843224005594", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "d1e4122", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e4123"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "d1e4125", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e4126", "view": "all"}, "$$": [{"#name": "__text__", "_": "We present the question answering engine GeoQA2 which is able to answer geospatial questions over the union of knowledge graphs YAGO2 and YAGO2geo. We also present the dataset "}, {"#name": "small-caps", "_": "GeoQuestions1089"}, {"#name": "__text__", "_": " which consists of 1089 natural language questions, their corresponding SPARQL or GeoSPARQL queries and their answers over the union of the same knowledge graphs. We use this dataset to compare the effectiveness of GeoQA2 and the system of Hamzei et al. 2022 and make it publicly available to be used by other researchers. Our evaluation shows that although the engine GeoQA2 performs better than the engine of Hamzei et al. 2022, both engines have ample room for improvement in their question answering performance."}]}]}]}]}}