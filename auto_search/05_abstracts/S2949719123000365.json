{"pii": "S2949719123000365", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e2280"}, "$$": [{"#name": "section-title", "$": {"id": "d1e2281"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e2283"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e2284"}, "_": "Lexical complexity prediction (LCP) determines the complexity level of words or phrases in a sentence. LCP has a significant impact on the enhancement of language translations, readability assessment, and text generation. However, the domain-specific technical word, the complex grammatical structure, the polysemy problem, the inter-word relationship, and dependencies make it challenging to determine the complexity of words or phrases. In this paper, we propose an integrated transformer regressor model named ITRM-LCP to estimate the lexical complexity of words and phrases where diverse contextual features are extracted from various transformer models. The transformer models are fine-tuned using the text-pair data. Then, a bidirectional LSTM-based regressor module is plugged on top of each transformer to learn the long-term dependencies and estimate the complexity scores. The predicted scores of each module are then aggregated to determine the final complexity score. We assess our proposed model using two benchmark datasets from shared tasks. Experimental findings demonstrate that our ITRM-LCP model obtains 10.2% and 8.2% improvement on the news and Wikipedia corpus of the CWI-2018 dataset, compared to the top-performing systems (DAT, CAMB, and TMU). Additionally, our ITRM-LCP model surpasses state-of-the-art LCP systems (DeepBlueAI, JUST-BLUE) by 1.5% and 1.34% for single and multi-word LCP tasks defined in the SemEval LCP-2021 task."}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "view": "all", "id": "d1e2286"}, "$$": [{"#name": "section-title", "$": {"id": "d1e2287"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e2289"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e2290"}, "$$": [{"#name": "list", "$": {"id": "d1e2292"}, "$$": [{"#name": "list-item", "$": {"id": "d1e2293"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2296"}, "_": "Lexical complexity prediction is a subtask of text simplification."}]}, {"#name": "list-item", "$": {"id": "d1e2298"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2301"}, "_": "Fusion of transformer models provides more meaningful representations of the inputs."}]}, {"#name": "list-item", "$": {"id": "d1e2303"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2306"}, "_": "The BiLSTM-Regressor improve pairwise learning between sentence and target word."}]}, {"#name": "list-item", "$": {"id": "d1e2308"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"view": "all", "id": "d1e2311"}, "_": "Contextual features from transformers effective for lexical complexity estimation."}]}]}]}]}]}]}}