{"pii": "S2352711022001881", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "view": "all", "id": "d1e197"}, "$$": [{"#name": "section-title", "$": {"id": "d1e198"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"view": "all", "id": "d1e200"}, "$$": [{"#name": "simple-para", "$": {"view": "all", "id": "d1e201"}, "$$": [{"#name": "__text__", "_": "The evaluation of clustering algorithms can involve running them on a variety of benchmark problems, and comparing their outputs to the reference, ground-truth groupings provided by experts. Unfortunately, many research papers and graduate theses consider only a small number of datasets. Also, the fact that there can be many equally valid ways to cluster a given problem set is rarely taken into account. In order to overcome these limitations, we have developed a framework whose aim is to introduce a consistent methodology for testing clustering algorithms. Furthermore, we have aggregated, polished, and standardised many clustering benchmark dataset collections referred to across the machine learning and data mining literature, and included new datasets of different dimensionalities, sizes, and cluster types. An interactive datasets explorer, the documentation of the Python API, a description of the ways to interact with the framework from other programming languages such as R or MATLAB, and other details are all provided at "}, {"#name": "inter-ref", "$": {"xmlns:xlink": true, "id": "interref1", "href": "https://clustering-benchmarks.gagolewski.com", "type": "simple"}, "_": "https://clustering-benchmarks.gagolewski.com"}, {"#name": "__text__", "_": "."}]}]}]}]}}