{"pii": "S0140366423003602", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "attachments", "$$": [{"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S0140366423003602-si1.svg"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0140366423003602/image/svg+xml/028e38664f4660665bece8b2d1cf7eec/si1.svg"}, {"#name": "file-basename", "_": "si1"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "si1.svg"}, {"#name": "extension", "_": "svg"}, {"#name": "filesize", "_": "2093"}, {"#name": "attachment-type", "_": "ALTIMG"}]}, {"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S0140366423003602-si2.svg"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0140366423003602/image/svg+xml/7a16abdb2433409d7535e990905976d5/si2.svg"}, {"#name": "file-basename", "_": "si2"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "si2.svg"}, {"#name": "extension", "_": "svg"}, {"#name": "filesize", "_": "2721"}, {"#name": "attachment-type", "_": "ALTIMG"}]}]}, {"#name": "abstract", "$": {"class": "author", "id": "d1e588", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "d1e589"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "d1e591", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "d1e592", "view": "all"}, "$$": [{"#name": "__text__", "_": "Network caching in Vehicular Named Data Networks (VNDN) has the potential to support latency-sensitive services, though given the massive amount of content generated by vehicles in a VNDN, it is challenging to cache sufficiently diverse content across the network for an acceptable hit ratio. Redundant interest transmissions can negatively impact network overhead, wasting network resources when retrieving duplicate content. In the literature, a caching decision is typically based on ongoing popularity, computed from a vehicle\u2019s experience with requests received for individual content. This leads to each vehicle caching the most popular content. Instead, we group content into different categories. All content in a popular category of vehicular service (e.g., tourist information category) is cached equally, not just the most popular content within a category (e.g., information relating to a particular museum). This increases content diversity in the cache. Further, we posit that a service requester from a particular location at a specific time would be interested in the same category of service/application as other users from the same location/time. Leveraging this observation, we consider both forthcoming spatial/temporal and ongoing popularity of vehicular services when ranking their popularity. We also address network overhead with a mobility model that captures the communication duration between vehicles. This parameter, which has been overlooked in existing literature, enables the selection of an appropriate forwarder for content requests. We use ndnSIM VNDN with a mobility trace of Luxembourg city for simulation. Compared to an approach where individual content-wise popularity is computed, our scheme shows more than 100% improvement in cache hit ratio where the number of contents in the network varied between "}, {"#name": "math", "$": {"xmlns:mml": true, "altimg": "si1.svg", "display": "inline", "id": "d1e595"}, "$$": [{"#name": "mrow", "$$": [{"#name": "mn", "_": "1"}, {"#name": "msup", "$$": [{"#name": "mrow", "$$": [{"#name": "mn", "_": "0"}]}, {"#name": "mrow", "$$": [{"#name": "mn", "_": "4"}]}]}]}]}, {"#name": "__text__", "_": " and "}, {"#name": "math", "$": {"xmlns:mml": true, "altimg": "si2.svg", "display": "inline", "id": "d1e608"}, "$$": [{"#name": "mrow", "$$": [{"#name": "mn", "_": "1"}, {"#name": "msup", "$$": [{"#name": "mrow", "$$": [{"#name": "mn", "_": "0"}]}, {"#name": "mrow", "$$": [{"#name": "mn", "_": "5"}]}]}]}]}, {"#name": "__text__", "_": ". Simultaneously, we also enhanced the diversity of content caching, while considering the trade-off between cache hit ratio and network overhead."}]}]}]}]}}