{"pii": "S1877050916311516", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"id": "abs0005", "view": "all", "class": "author"}, "$$": [{"#name": "section-title", "$": {"id": "sect0005"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "abst0005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "spar0005", "view": "all"}, "_": "This paper proposes an efficient and scalable method for concept extraction and concept hierarchy learning from large unstructured text corpus which is guided by a topic modeling process. The method leverages \u201cconcepts\u201d from statistically discovered \u201ctopics\u201d and then learns a hierarchy of those concepts by exploiting a subsumption relation between them. Advantage of the proposed method is that the entire process falls under the unsupervised learning paradigm thus the use of a domain specific training corpus can be eliminated. Given a massive collection of text documents, the method maps topics to concepts by some lightweight statistical and linguistic processes and then probabilistically learns the subsumption hierarchy. Extensive experiments with large text corpora such as BBC News dataset and Reuters News corpus shows that our proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a topic modeling process."}]}]}]}}