{"pii": "S2215098619314077", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "abstract", "$": {"class": "author", "id": "ab005", "lang": "en", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st005"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0005", "view": "all"}, "_": "Developing deep learning models that can scale to large image repositories is increasingly gaining significant efforts in the domain of image search. The current deep neural networks rely on the computational power of accelerators (e.g. GPUs) to tackle the processing limitations associated with features extraction and model training. This paper introduces and investigates a deep model of Convolutional Neural Networks (CNNs) to efficiently extract, index, and retrieve images in the context of large-scale Content-Based Image Retrieval (CBIR). Random Maclaurin projection is used to generate low-dimensional image descriptors and their discriminating efficiency is evaluated on standard image datasets. The scalability of deep architectures is also evaluated on one million image dataset over a High-Performance Computing (HPC) platform, which is assessed in terms of the retrieval accuracy, speed of features extraction and memory costs. Additionally, the controlling GPU kernels of the proposed model are examined under several optimization factors to evaluate their impact on the processing and retrieval performance. The experimental results show the effectiveness of the proposed model in the retrieval accuracy, GPU utilisation, speed of features extraction, and storage of image indexing."}]}]}]}}