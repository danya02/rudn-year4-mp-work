{"pii": "S0031320323003461", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "attachments", "$$": [{"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S0031320323003461-ga1.jpg"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0031320323003461/ga1/DOWNSAMPLED/image/jpeg/ab5ce574668b2327701ebc3c62e2be86/ga1.jpg"}, {"#name": "file-basename", "_": "ga1"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "ga1.jpg"}, {"#name": "extension", "_": "jpg"}, {"#name": "filesize", "_": "40674"}, {"#name": "pixel-height", "_": "200"}, {"#name": "pixel-width", "_": "353"}, {"#name": "attachment-type", "_": "IMAGE-DOWNSAMPLED"}]}, {"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S0031320323003461-ga1.sml"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0031320323003461/ga1/THUMBNAIL/image/gif/d4552e7286d0684e94726cd1ef56e3ef/ga1.sml"}, {"#name": "file-basename", "_": "ga1"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "ga1.sml"}, {"#name": "extension", "_": "sml"}, {"#name": "filesize", "_": "15811"}, {"#name": "pixel-height", "_": "124"}, {"#name": "pixel-width", "_": "219"}, {"#name": "attachment-type", "_": "IMAGE-THUMBNAIL"}]}, {"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S0031320323003461-ga1_lrg.jpg"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0031320323003461/HIGHRES/image/jpeg/f2c181bcb67b3a0dcc20b51578c1116d/ga1_lrg.jpg"}, {"#name": "file-basename", "_": "ga1"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "ga1_lrg.jpg"}, {"#name": "extension", "_": "jpg"}, {"#name": "filesize", "_": "368643"}, {"#name": "pixel-height", "_": "886"}, {"#name": "pixel-width", "_": "1565"}, {"#name": "attachment-type", "_": "IMAGE-HIGH-RES"}]}]}, {"#name": "abstract", "$": {"class": "author-highlights", "id": "absh001", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "sctt0001"}, "_": "Highlights"}, {"#name": "abstract-sec", "$": {"id": "abssec0001", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0005", "view": "all"}, "$$": [{"#name": "list", "$": {"id": "lst0001"}, "$$": [{"#name": "list-item", "$": {"id": "lstitem0001"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0001", "view": "all"}, "_": "We propose a novel self-supervised de-attention mechanism that prevents the network from focusing on non-static objects in visual place recognition tasks."}]}, {"#name": "list-item", "$": {"id": "lstitem0002"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0002", "view": "all"}, "_": "We also present a new sharpened triplet marginal loss that enhances the separation between the distributions of positive and negative descriptors."}]}, {"#name": "list-item", "$": {"id": "lstitem0003"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0003", "view": "all"}, "_": "To validate our methods in an extremely crowded environment, we introduce a new clutter augmentation method that creates crowded versions of the public benchmark datasets."}]}, {"#name": "list-item", "$": {"id": "lstitem0004"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0004", "view": "all"}, "_": "Experimental results demonstrate significant improvements over existing attention methods for visual geo-localization tasks."}]}, {"#name": "list-item", "$": {"id": "lstitem0005"}, "$$": [{"#name": "label", "_": "\u2022"}, {"#name": "para", "$": {"id": "p0005", "view": "all"}, "_": "Our de-attention module can be incorporated into any network, allowing users to selectively block unwanted clutter features for better performance."}]}]}]}]}]}, {"#name": "abstract", "$": {"class": "graphical", "id": "absh0002", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "sctt0002f"}, "_": "Graphical abstract"}, {"#name": "abstract-sec", "$": {"id": "abssec0002d", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0006n", "view": "all"}, "$$": [{"#name": "display", "$$": [{"#name": "figure", "$": {"id": "fig0014"}, "$$": [{"#name": "link", "$": {"xmlns:xlink": true, "id": "celink0014", "locator": "ga1", "type": "simple", "role": "http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4", "href": "pii:S0031320323003461/ga1"}}]}]}]}]}]}, {"#name": "abstract", "$": {"id": "abs0001", "class": "author", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "sctt0002"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "abssec0002", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp0006", "view": "all"}, "$$": [{"#name": "__text__", "_": "Thanks to Earth-level Street View images from Google Maps, a visual image geo-localization can estimate the coarse location of a query image with a visual place recognition process. However, this can get very challenging when non-static objects change with time, severely degrading image retrieval accuracy. We address the problem of city-scale visual place recognition in complex urban environments crowded with non-static clutters. To this end, we first analyze what clutters degrade similarity matching between the query and database images. Second, we design a self-supervised trainable de-attention module that prevents the network from focusing on non-static objects in an input image. In addition, we propose a novel triplet marginal loss called "}, {"#name": "italic", "_": "sharpened triplet marginal loss"}, {"#name": "__text__", "_": " to make feature descriptors more discriminative. Lastly, due to the lack of geo-tagged public datasets with a high density of non-static objects, we propose a clutter augmentation method to evaluate our approach. The experimental results show that our model has notably improved over the existing attention methods in geo-localization tasks on the public benchmark datasets and on their augmented versions with high population and traffic. Our code is available at "}, {"#name": "inter-ref", "$": {"xmlns:xlink": true, "id": "intrrf0001", "href": "https://github.com/ccsmm78/deattention_with_stml_for_vpr", "type": "simple"}, "_": "https://github.com/ccsmm78/deattention_with_stml_for_vpr"}, {"#name": "__text__", "_": "."}]}]}]}]}}