{"pii": "S131915782300126X", "abstracts": {"#name": "abstracts", "$": {"xmlns:ce": true, "xmlns:dm": true, "xmlns:sb": true}, "$$": [{"#name": "attachments", "$$": [{"#name": "attachment", "$": {"xmlns:xocs": true}, "$$": [{"#name": "attachment-eid", "_": "1-s2.0-S131915782300126X-si13.svg"}, {"#name": "ucs-locator", "_": "https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S131915782300126X/STRIPIN/image/svg+xml/b68d66ff2d5c9407193ff4e4dc64f45b/si13.svg"}, {"#name": "file-basename", "_": "si13"}, {"#name": "abstract-attachment", "_": "true"}, {"#name": "filename", "_": "si13.svg"}, {"#name": "extension", "_": "svg"}, {"#name": "filesize", "_": "1725"}, {"#name": "attachment-type", "_": "ALTIMG"}]}]}, {"#name": "abstract", "$": {"class": "author", "lang": "en", "id": "ab005", "view": "all"}, "$$": [{"#name": "section-title", "$": {"id": "st135"}, "_": "Abstract"}, {"#name": "abstract-sec", "$": {"id": "as005", "view": "all"}, "$$": [{"#name": "simple-para", "$": {"id": "sp005", "view": "all"}, "$$": [{"#name": "__text__", "_": "Grammatical Error Correction (GEC) refers to the automatic identification and amendment of grammatical, spelling, punctuation, and word-positioning errors in monolingual texts. Neural Machine Translation (NMT) is nowadays one of the most valuable techniques used for GEC but it may suffer from scarcity of training data and domain shift, depending on the addressed language. However, current techniques (e.g., tuning pre-trained language models or developing spell-confusion methods without focusing on language diversity) tackling the data sparsity problem associated with NMT create mismatched data distributions. This paper proposes new aggressive transformation approaches to augment data during training that extend the distribution of authentic data. In particular, it uses augmented data as auxiliary tasks to provide new contexts when the target prefix is not helpful for the next word prediction. This enhances the encoder and steadily increases its contribution by forcing the GEC model to pay more attention to the text representations of the encoder during decoding. The impact of these approaches was investigated using the Transformer-based for low-resource GEC task, and Arabic GEC was used as a case study. GEC models trained with our data tend more to source information, are more domain shift robustness, and have less hallucinations with tiny training datasets and domain shift. Experimental results showed that the proposed approaches outperformed the baseline, the most common data augmentation methods, and classical synthetic data approaches. In addition, a combination of the three best approaches "}, {"#name": "italic", "_": "Misspelling"}, {"#name": "__text__", "_": ", "}, {"#name": "italic", "_": "Swap"}, {"#name": "__text__", "_": ", and "}, {"#name": "italic", "_": "Reverse"}, {"#name": "__text__", "_": " achieved the best "}, {"#name": "math", "$": {"xmlns:mml": true, "altimg": "si13.svg"}, "$$": [{"#name": "mrow", "$$": [{"#name": "msub", "$$": [{"#name": "mrow", "$$": [{"#name": "mi", "_": "F"}]}, {"#name": "mrow", "$$": [{"#name": "mn", "_": "1"}]}]}]}]}, {"#name": "__text__", "_": " score in two benchmarks and outperformed previous Arabic GEC approaches."}]}]}]}]}}